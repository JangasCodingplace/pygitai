from pathlib import Path
from typing import Type

from pygitai.common import llm
from pygitai.common.config import config
from pygitai.common.llm.base import LLMBase, PromptLine
from pygitai.common.logger import get_logger
from pygitai.common.utils import camel_to_snake, load_template_file

from .base_job import BaseJob

logger = get_logger(__name__, config.logger.level)


class NoJobConfigured(Exception):
    """No job configured"""


class ImproperlyConfigured(Exception):
    """Improperly configured"""


def ask_for_user_feedback(prompt_output_context: str, prompt_output: str):
    """Ask the user for feedback"""
    logger.info(f"Prompt Output for {prompt_output_context}: {prompt_output}")
    agree = input("Do you agree with the prompt output? [y/n]")
    if agree.lower() == "y":
        return "y"
    elif agree.lower() == "n":
        recommendation = input("Any recommendation for a better output?")
        return recommendation
    else:
        logger.warn("Wrong input. Please only enter 'y' or 'n'")
        return ask_for_user_feedback(prompt_output_context, prompt_output)


class LLMJobBase(BaseJob):
    """Base class for LLMJob. This kind of job should be used if an
    interaction with a LLM is required.

    Attributes:
        llm (Type[LLMBase] | None): The LLM API that should be used.
            If it's none the default LLM API or the specified LLM API
            in the config will be used.
        llm_model (str | None): The LLM model that should be used. If
            it's none the default LLM model or the specified LLM model
            in the config will be used.
        template_file (Path | str | None): The path to the template
            file that should be used. If it's none the default
            template file or the specified template file will be
            used.
    """

    llm: Type[LLMBase] | None = None
    llm_model: str | None = None
    template_file: Path | str | None = None

    @property
    def context(self):
        return camel_to_snake(self.__class__.__name__)

    def get_llm_initial_message(
        self,
        context_system: dict | None = None,
        context_user: dict | None = None,
    ):
        """Get the initial message from the LLM.

        This method will use the template files to generate the
        initial message.

        A prompt consists of two parts: The system part and the user
        part. The system part just contains generic information about
        the context. The user part contains the information that is
        specific to the user (i.e. current code changes).

        This method will be always used to start a conversation with
        the LLM.

        Arguments:
            context_system (dict | None): Additional context which
                will be passed to the template file for the system
            context_user (dict | None): Additional context which will
                be passed to the template file for the user
        """
        content_system = load_template_file(
            template_path=self.get_template_file(type_="system"),
            context=context_system or {},
        )
        content_user = load_template_file(
            template_path=self.get_template_file(type_="user"),
            context=context_user or {},
        )
        prompt = self.get_llm_klass().llm_parser.parse_prompt(
            input_data=(
                PromptLine(role="system", text=content_system),
                PromptLine(role="user", text=content_user),
            )
        )
        return prompt

    def get_llm_response(
        self,
        context_system: dict | None = None,
        context_user: dict | None = None,
        prompt_override: list = None,
    ):
        """Get the response from the LLM.

        This method will use a LLM API and execute a prompt. The
        response of the LLM will be returned.

        The prompt will be generated by the `get_llm_initial_message`
        method.

        Arguments:
            context_system (dict | None): Additional context which
                will be passed to the template file for the system
            context_user (dict | None): Additional context which will
                be passed to the template file for the user
            prompt_override (list | None): This parameter can be set
                if there is an already existing prompt or
                conversation which should be continued. If it's None
                the prompt will be generated by the
                `get_llm_initial_message` method.
        """
        prompt = prompt_override or self.get_llm_initial_message(
            context_system=context_system or {},
            context_user=context_user or {},
        )
        return self.get_llm_klass().exec_prompt(
            prompt=prompt,
            model=self.get_llm_model(),
        )

    def process_user_feedback_llm_loop(
        self,
        context: str,
        context_user: dict | None = None,
        context_system: dict | None = None,
    ):
        """Process user llm interaction in an infinite loop.

        This method is an interactive method. It's moderating the
        interaction between the user and the LLM until the user is
        satisfied with the output.

        Arguments:
            context (str): The context of the interaction. This will
                be used to determine the template file that should be
                used. This argument will be deprecated in the future.
            context_system (dict | None): Additional context which
                will be passed to the template file for the system
            context_user (dict | None): Additional context which will
                be passed to the template file for the user
        """
        prompt_override = None
        user_feedback = None
        while user_feedback != "y":
            prompt_output, prompt_output_full_context = self.get_llm_response(
                prompt_override=prompt_override,
                context_user=context_user or {},
                context_system=context_system or {},
            )
            user_feedback = ask_for_user_feedback(
                prompt_output_context=context,
                prompt_output=prompt_output,
            )
            if user_feedback != "y":
                revision_prompt = load_template_file(
                    template_path=self.get_template_file(type_="revision"),
                    context={"feedback": user_feedback or "No further info provided"},
                )
                prompt_override = (
                    prompt_output_full_context
                    + self.get_llm_klass().llm_parser.parse_prompt(
                        input_data=(PromptLine(role="system", text=revision_prompt),)
                    )
                )
        return prompt_output

    def get_llm_klass(self) -> Type[LLMBase]:
        """Get the LLM API that should be used.

        This method will use the `llm_api` attribute of the job
        instance. If it's not set, the default LLM API or the
        specified LLM API in the config will be used.
        """
        if self.llm is not None:
            return self.llm

        llm_api_name = None
        cfg_ = config.general.cfg
        if f"pygitai.jobs.{self.__class__.__name__}" in cfg_:
            llm_api_name = cfg_[f"pygitai.jobs.{self.__class__.__name__}"].get(
                "llm_api"
            )
        if not llm_api_name:
            llm_api_name = cfg_["pygitai"].get("default_llm_api")
        if not llm_api_name:
            raise NoJobConfigured(
                f"No LLM API configured for job {self.__class__.__name__}"
            )
        return getattr(llm, llm_api_name)

    def get_llm_model(self) -> str:
        """Get the LLM model that should be used.

        This method will use the `llm_model` attribute of the job
        instance. If it's not set, the default LLM model or the
        specified LLM model in the config will be used.
        """
        cfg_ = config.general.cfg
        llm_model = None
        if f"pygitai.jobs.{self.__class__.__name__}" in cfg_:
            return cfg_[f"pygitai.jobs.{self.__class__.__name__}"].get("llm_model")
        if not llm_model:
            return cfg_["pygitai"].get("default_llm_model")
        else:
            raise ImproperlyConfigured(
                f"No LLM Model configured for job {self.__class__.__name__}"
            )

    def get_template_file(self, type_: str) -> Path:
        """Get the template file that should be used.

        This method will use the `template_file` attribute of the job
        instance. If it's not set, the default template file or the
        specified template file in the config will be used.

        Arguments:
            type_ (str): The type of the template file. This will be
                used to determine the template file name. Allowed
                values are: system, user, revision
        """
        if self.template_file is not None:
            if isinstance(self.template_file, Path):
                return self.template_file
            return Path(self.template_file)

        template_file_path = None
        template_dir = None
        template_file_name = f"{self.context}_{type_}.txt"
        cfg_ = config.general.cfg

        if f"pygitai.jobs.{self.__class__.__name__}" in cfg_:
            template_dir = cfg_[f"pygitai.jobs.{self.__class__.__name__}"].get(
                "prompt_template_dir"
            )

        if not template_file_path:
            template_dir = cfg_["pygitai"].get("default_prompt_template_dir")

        if not template_dir:
            raise NoJobConfigured(
                f"No LLM API configured for job {self.__class__.__name__}"
            )

        template_dir_path = config.general.toplevel_directory / Path(template_dir)
        template_file_path = template_dir_path / template_file_name
        if not template_file_path.exists():
            raise ImproperlyConfigured(
                f"No template file configured for job {self.__class__.__name__}"
            )

        return template_file_path

    def exec_command(self, *args, **kwargs):
        """Execute the command"""
        raise NotImplementedError
